{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Iterator, Generator\n",
    "\n",
    "# import music21\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "score_names = np.load('./data/score_names.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5930/5930 [00:10<00:00, 564.12it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = [\n",
    "    (np.load('./data/{}.npy'.format(i)))\n",
    "    for i in tqdm(range(len(score_names)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5930/5930 [00:17<00:00, 330.00it/s]\n"
     ]
    }
   ],
   "source": [
    "meta = [\n",
    "    (np.load('./data/{}_meta.npy'.format(i)))\n",
    "    for i in tqdm(range(len(score_names)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pitch = np.max([np.max(t) for t in scores])\n",
    "min_pitch = np.min([np.min(t[t > 0]) for t in scores])\n",
    "n_notes = int(max_pitch - min_pitch) + 1\n",
    "idx_slur = 0\n",
    "idx_beat = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "SEQUENCE_STEPS = 4  # The number of windows to look at; 4 * 32 means we look at an 8-bar window\n",
    "window_size = 32  # each measure is 16, so 32 is a two-measure window\n",
    "n_features = n_notes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_targets(score, voice):\n",
    "    \"\"\"\n",
    "    Make our target variables. It is the a stream of notes and one of metadata\n",
    "    for a specified voice in the score.\n",
    "    \"\"\"\n",
    "    n_output_features = n_notes + 1\n",
    "    y = np.zeros((score.shape[1], n_output_features))  # shape: n timesteps X m features\n",
    "    for i, note in enumerate(score[voice]):\n",
    "        if note > 0:\n",
    "            note_idx = int(note - min_pitch)\n",
    "            y[i, note_idx + 1] = 1\n",
    "        else:\n",
    "            y[i, 0] = 1 # it's a rest\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_targets_slur(meta, voice):\n",
    "    \"\"\"\n",
    "    Make our target variables. It is the a stream of notes and one of metadata\n",
    "    for a specified voice in the score.\n",
    "    \"\"\"\n",
    "    return meta[voice, :, idx_slur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded(score, window_size, max_voices=None):\n",
    "    # pad the beginning of the sequence so that our first window ends on the first timestep\n",
    "    # also padd the voices\n",
    "    padding_size = window_size - 1\n",
    "\n",
    "    score_padding = np.zeros((score.shape[0], padding_size))\n",
    "    return np.hstack((score_padding, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_beat(meta, voice):\n",
    "    return meta[voice, :, idx_beat:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_sequence(score, meta, voice, sequence_steps=16, conv_window_size=32):\n",
    "    \"\"\"\n",
    "    Make an input sequence for a particular voice\n",
    "    \"\"\"\n",
    "    window_size = sequence_steps * conv_window_size\n",
    "    # First, do the notes channel\n",
    "    padded_score = make_padded(score, window_size) / max_pitch\n",
    "    padding_size = window_size - 1\n",
    "    \n",
    "    # Now, the slurs channel\n",
    "    padded_meta = make_padded(meta[:, :, 0], window_size)\n",
    "    \n",
    "    # A mask showing which voice to predict\n",
    "    voice_mask = np.zeros(padded_meta.shape)\n",
    "    \n",
    "    # Stack them together\n",
    "    indexer = np.arange(window_size)[None, :] + np.arange(padded_score.shape[1] - padding_size)[:, None]\n",
    "    stacked = np.stack((padded_score, padded_meta, voice_mask), axis=-1)\n",
    "    \n",
    "    # Make the sliding windows\n",
    "    sequence = stacked.swapaxes(0, 1)[indexer, :, :]\n",
    "    \n",
    "    # Now, mask out the target values\n",
    "    sequence[:, -1, voice, :1] = 0\n",
    "    \n",
    "    # Set a flag in the voice mask to indicate which voice is to be predicted\n",
    "    sequence[:, -1, voice, 2] = 1\n",
    "    \n",
    "    return sequence.reshape((score.shape[1], -1, conv_window_size, padded_score.shape[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train, scores_valid, meta_train, meta_valid = train_test_split(scores, meta, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880, 16, 32, 5, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_input_sequence(scores_valid[0], meta_valid[0], 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSequence(Sequence):\n",
    "    def __init__(self, scores, meta, subsample_voices=False):\n",
    "        self.scores = scores\n",
    "        self.meta = meta\n",
    "        if subsample_voices:\n",
    "            # Take one randomly sampled voice for each score\n",
    "            voice_sample = [\n",
    "                np.random.randint(score.shape[0]) \n",
    "                for score in scores\n",
    "            ]\n",
    "            self.indices = [\n",
    "                (score_idx, voice_sample[score_idx])\n",
    "                for score_idx, score in enumerate(scores)\n",
    "            ]\n",
    "        else:\n",
    "            self.indices = [\n",
    "                (score_idx, voice_idx)\n",
    "                for score_idx, score in enumerate(scores)\n",
    "                for voice_idx in range(score.shape[0])\n",
    "            ]\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        score_idx, voice = self.indices[idx]\n",
    "        score = self.scores[score_idx]\n",
    "        meta = self.meta[score_idx]\n",
    "        return (\n",
    "            [\n",
    "                make_input_sequence(score, meta, voice, sequence_steps=SEQUENCE_STEPS, conv_window_size=window_size),\n",
    "                make_input_beat(meta, voice)\n",
    "            ],\n",
    "            [\n",
    "                make_targets(score, voice),\n",
    "                make_targets_slur(meta, voice)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sequence = BatchSequence(scores_valid, meta_valid, subsample_voices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = BatchSequence(scores_train, meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras.activations import relu\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "conv_lst_m2d_6_input (InputLayer (None, None, 32, None 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)      (None, None, 32, None 40448       conv_lst_m2d_6_input[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)   (None, None, 16, None 0           conv_lst_m2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)      (None, 16, None, 64)  221440      max_pooling3d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glob (None, 64)            0           conv_lst_m2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 16)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 80)            0           global_average_pooling2d_3[0][0] \n",
      "                                                                   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 100)           8100        concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 59)            5959        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             101         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 276,048\n",
      "Trainable params: 276,048\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "notes_model = Sequential()\n",
    "notes_model.add(layers.ConvLSTM2D(32, 3, return_sequences=True, padding='same', input_shape=(None, window_size, None, 3)))\n",
    "notes_model.add(layers.MaxPool3D(2, 2))\n",
    "notes_model.add(layers.ConvLSTM2D(64, 3, padding='same'))\n",
    "notes_model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "beats_input = layers.Input(shape=(16,))\n",
    "\n",
    "features = layers.concatenate([notes_model.output, beats_input])\n",
    "\n",
    "fc_1 = layers.Dense(100, activation='relu')(features)\n",
    "dropout = layers.Dropout(0.1)(fc_1)\n",
    "\n",
    "output_notes = layers.Dense(n_notes + 1, activation='softmax')(dropout)\n",
    "output_slur = layers.Dense(1, activation='sigmoid')(fc_1)\n",
    "\n",
    "model = Model(inputs=[notes_model.input, beats_input], outputs=[output_notes, output_slur])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "\n",
    "top3_acc.__name__ = 'top3_acc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss=['categorical_crossentropy', 'binary_crossentropy'], metrics=['accuracy', top3_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='./models/model6.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25420"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(train_sequence)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_steps = len(valid_sequence)\n",
    "validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no weights found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights('./models/model6.hdf5')\n",
    "except (OSError, ValueError):\n",
    "    print('no compatible weights found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ben.cohen/miniconda3/envs/palestrinet/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_sequence,\n",
    "    steps_per_epoch=len(train_sequence),\n",
    "    epochs=5,\n",
    "    validation_data=valid_sequence,\n",
    "    validation_steps=len(valid_sequence),\n",
    "    callbacks=[checkpointer],\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv_lst_m2d_1_input to have shape (None, None, 32, None, 2) but got array with shape (880, 4, 32, 5, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/palestrinet/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/palestrinet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2174\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2176\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/palestrinet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1796\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/palestrinet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/palestrinet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv_lst_m2d_1_input to have shape (None, None, 32, None, 2) but got array with shape (880, 4, 32, 5, 3)"
     ]
    }
   ],
   "source": [
    "%time pred = model.evaluate_generator(valid_sequence, steps=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./models/model6.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_pred = np.argmax(pred[0][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = np.argmax(valid_sequence[0][1][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  66.,  66.,  66.,  66.,  66.,  66.,\n",
       "        66.,  66.,  67.,  67.,  67.,  67.,  71.,  71.,  71.,  71.,  71.,\n",
       "        71.,  71.,  71.,  69.,  69.,  69.,  69.,  67.,  67.,  67.,  67.,\n",
       "        66.,  66.,  66.,  66.,  64.,  64.,  64.,  64.,  67.,  67.,  67.,\n",
       "        67.,  67.,  67.,  67.,  67.,  66.,  66.,  66.,  66.,  64.,  64.,\n",
       "        64.,  64.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  66.,  66.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,\n",
       "        60.,  60.,  60.,  60.,  60.,  60.,  60.,  60.,  62.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        62.,  62.,  60.,  60.,  60.,  60.,  62.,  62.,  62.,  62.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        57.,  57.,  57.,  57.,  59.,  59.,  59.,  59.,  59.,  59.,  59.,\n",
       "        59.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,  67.,\n",
       "        67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  60.,  60.,  60.,\n",
       "        60.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  63.,  63.,\n",
       "        63.,  63.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,   0.,   0.,   0.,   0.,  57.,  57.,  57.,\n",
       "        57.,  65.,  65.,  65.,  65.,  65.,  65.,  65.,  65.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  62.,  62.,  62.,  62.,  67.,  67.,  67.,  67.,\n",
       "        66.,  66.,  66.,  66.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,\n",
       "        64.,  66.,  66.,  66.,  66.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,  67.,\n",
       "        67.,  67.,  67.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,\n",
       "        67.,  67.,  67.,  67.,  66.,  66.,  66.,  66.,  66.,  66.,  66.,\n",
       "        66.,  67.,  67.,  67.,  67.,  67.,  67.,  66.,  66.,  64.,  64.,\n",
       "        62.,  62.,  60.,  60.,  59.,  59.,  60.,  60.,  60.,  60.,  60.,\n",
       "        60.,  60.,  60.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,\n",
       "        59.,  59.,  59.,  59.,  59.,  59.,  59.,  59.,  59.,  59.,  59.,\n",
       "        59.,  60.,  60.,  60.,  60.,  62.,  62.,  62.,  62.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  64.,\n",
       "        64.,  64.,  64.,  65.,  65.,  65.,  65.,  67.,  67.,  65.,  65.,\n",
       "        64.,  64.,  62.,  62.,  60.,  60.,  60.,  60.,  60.,  60.,  60.,\n",
       "        60.,  60.,  60.,  60.,  60.,  60.,  60.,  60.,  60.,  65.,  65.,\n",
       "        65.,  65.,  65.,  65.,  65.,  65.,  65.,  65.,  65.,  65.,  62.,\n",
       "        62.,  62.,  62.,  60.,  60.,  60.,  60.,  57.,  57.,  57.,  57.,\n",
       "        59.,  59.,  59.,  59.,  59.,  59.,  62.,   0.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,   0.,   0.,   0.,   0.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  66.,  66.,  66.,  66.,  67.,  67.,  67.,  67.,\n",
       "        67.,  67.,  66.,  66.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  64.,  64.,  64.,  64.,\n",
       "        62.,  62.,  62.,  62.,  60.,  60.,  60.,  60.,  62.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  60.,  60.,  59.,  59.,  59.,  59.,\n",
       "        57.,  57.,  57.,  57.,  59.,  59.,  59.,  59.,  59.,  59.,  59.,\n",
       "        59.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  69.,  69.,\n",
       "        69.,  69.,  69.,  69.,  67.,  67.,  66.,  66.,  66.,  66.,  64.,\n",
       "        64.,  64.,  64.,  66.,  66.,  66.,  66.,  66.,  66.,  66.,  66.,\n",
       "        64.,  64.,  64.,  64.,  59.,  59.,  59.,  59.,  61.,  61.,  61.,\n",
       "        61.,  61.,  61.,  61.,  61.,  62.,  62.,  62.,  62.,  62.,  62.,\n",
       "        60.,  60.,  59.,  59.,  59.,  59.,  55.,  55.,  55.,  55.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        64.,  64.,  64.,  64.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,\n",
       "        59.,  59.,  59.,  59.,  67.,  67.,  67.,  67.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  66.,  66.,  66.,  66.,  67.,  67.,\n",
       "        67.,  67.,  69.,  69.,  69.,  69.,  66.,  66.,  66.,  66.,  64.,\n",
       "        64.,  64.,  64.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,   0.,   0.,   0.,   0.,  59.,  59.,  59.,\n",
       "        59.,  60.,  60.,  60.,  60.,  60.,  60.,  60.,  60.,  62.,  62.,\n",
       "        62.,  62.,  62.,  62.,  62.,  62.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  66.,  66.,  66.,  66.,  67.,  67.,  67.,\n",
       "        67.,  67.,  67.,  67.,  67.,  65.,  65.,  65.,  65.,  65.,  65.,\n",
       "        65.,  65.,   0.,   0.,   0.,   0.,  59.,  59.,  59.,  59.,  60.,\n",
       "        60.,  60.,  60.,  60.,  60.,  62.,  62.,  64.,  64.,  64.,  64.,\n",
       "        69.,  69.,  69.,  69.,  69.,  69.,  67.,  67.,  65.,  65.,  64.,\n",
       "        64.,  62.,  62.,  62.,  62.,  62.,  62.,  64.,  64.,  65.,  65.,\n",
       "        65.,  65.,  65.,  65.,  65.,  65.,  64.,  64.,  64.,  64.,  64.,\n",
       "        64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.,  64.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes + (notes > 1) * (min_pitch - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_valid[0][2] - (notes + (notes > 1) * (min_pitch - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
